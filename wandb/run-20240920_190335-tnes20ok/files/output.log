C:\Users\coolm\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\utils\data\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
C:\Users\coolm\AppData\Local\Temp\ipykernel_29200\1507249952.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)
Unsupported operator aten::tanh encountered 3 time(s)
Unsupported operator aten::ones_like encountered 9 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 9 time(s)
Unsupported operator aten::log_softmax encountered 1 time(s)
Total trainable parameters of VietaPell: 105856
FLOPs of VietaPell: 106046.0
Training:   0%|          | 0/938 [00:00<?, ?it/s]C:\Users\coolm\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\utils\data\dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
C:\Users\coolm\AppData\Local\Temp\ipykernel_29200\1507249952.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)
Training: 100%|██████████| 938/938 [01:04<00:00, 14.47it/s, Loss=1.51, Acc=45.2]
Epoch 1/50:
Train Loss: 1.5081, Train Acc: 0.4516
Test Loss: 0.4777, Test Acc: 0.8535
Training: 100%|██████████| 938/938 [01:35<00:00,  9.82it/s, Loss=0.631, Acc=79.8]
Epoch 2/50:
Train Loss: 0.6295, Train Acc: 0.7981
Test Loss: 0.3460, Test Acc: 0.8934
Training: 100%|██████████| 938/938 [01:27<00:00, 10.68it/s, Loss=0.456, Acc=85.6]
Epoch 3/50:
Train Loss: 0.4557, Train Acc: 0.8555
Test Loss: 0.2155, Test Acc: 0.9357
Training: 100%|██████████| 938/938 [01:11<00:00, 13.04it/s, Loss=0.372, Acc=88.3]
Epoch 4/50:
Train Loss: 0.3705, Train Acc: 0.8835
Test Loss: 0.2016, Test Acc: 0.9380
Training: 100%|██████████| 938/938 [01:18<00:00, 11.91it/s, Loss=0.341, Acc=89.3]
Epoch 5/50:
Train Loss: 0.3412, Train Acc: 0.8925
Test Loss: 0.2311, Test Acc: 0.9266
Training: 100%|██████████| 938/938 [01:10<00:00, 13.26it/s, Loss=0.311, Acc=90.3]
Epoch 6/50:
Train Loss: 0.3094, Train Acc: 0.9025
Test Loss: 0.2215, Test Acc: 0.9303
Training: 100%|██████████| 938/938 [01:16<00:00, 12.29it/s, Loss=0.294, Acc=90.8]
Epoch 7/50:
Train Loss: 0.2936, Train Acc: 0.9078
Test Loss: 0.1845, Test Acc: 0.9409
Training: 100%|██████████| 938/938 [01:19<00:00, 11.87it/s, Loss=0.274, Acc=91.4]
Epoch 8/50:
Train Loss: 0.2727, Train Acc: 0.9139
Test Loss: 0.1622, Test Acc: 0.9487
Training:   4%|▍         | 38/938 [00:38<15:19,  1.02s/it, Loss=0.246, Acc=91.9] 
